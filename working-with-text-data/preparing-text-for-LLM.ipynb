{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1b26664c-1d89-4758-a8bd-82d40c34ab2f",
      "metadata": {
        "id": "1b26664c-1d89-4758-a8bd-82d40c34ab2f"
      },
      "source": [
        "# Working with Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b4dd9ff9-de5d-46c7-8a33-83518d0e84f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4dd9ff9-de5d-46c7-8a33-83518d0e84f1",
        "outputId": "4a137728-f18a-4d72-800c-a6ebd4b99b27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('roman-urdu-corpus.txt', <http.client.HTTPMessage at 0x7c579b60a060>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# load the data\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/ArsalanMateen/Mahir-on-Call/refs/heads/main/working-with-text-data/roman-urdu-corpus.txt\")\n",
        "file_path = \"roman-urdu-corpus.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "63a27551-2be2-4829-8226-a517afe51dc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63a27551-2be2-4829-8226-a517afe51dc6",
        "outputId": "4ac8fb25-726a-4233-bf48-aa11c37407f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of character: 9843914\n"
          ]
        }
      ],
      "source": [
        "with open(\"roman-urdu-corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"The total number of character:\", len(raw_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fa3dad8a-6095-48c0-8e03-df5d1bb3fc3a",
      "metadata": {
        "id": "fa3dad8a-6095-48c0-8e03-df5d1bb3fc3a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1 (Dataset):\n",
        "    def __init__(self, raw_text, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # tokenization using byte pair encoding\n",
        "        token_ids = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|\"})\n",
        "\n",
        "        # data sampling using sliding window\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i+1:i + max_length+1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d2b69481-3493-4a99-bcfb-96fe500af049",
      "metadata": {
        "id": "d2b69481-3493-4a99-bcfb-96fe500af049"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "def create_dataloader_v1(raw_text,\n",
        "                         batch_size,\n",
        "                         max_length,\n",
        "                         stride,\n",
        "                         shuffle = True,\n",
        "                         drop_last = True,\n",
        "                         num_workers = 0):\n",
        "\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(raw_text, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "74975229",
      "metadata": {
        "id": "74975229"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 768\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Initialize token embedding layer\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "734c1f67",
      "metadata": {
        "id": "734c1f67"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "max_length = 1024\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=batch_size,\n",
        "    max_length=max_length,\n",
        "    stride=max_length,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "135a4fa2",
      "metadata": {
        "id": "135a4fa2"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "# Initialize position embedding layer\n",
        "position_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0b2b66eb",
      "metadata": {
        "id": "0b2b66eb"
      },
      "outputs": [],
      "source": [
        "# creating input embeddings\n",
        "for batch in dataloader:\n",
        "    input_ids, target_ids = batch\n",
        "\n",
        "    token_embeddings = token_embedding_layer(input_ids)\n",
        "    position_embeddings = position_embedding_layer(torch.arange(context_length))\n",
        "\n",
        "    input_embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c2afb066",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2afb066",
        "outputId": "07990d24-5aed-479f-a403-52fce4e6408d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1024, 768])\n"
          ]
        }
      ],
      "source": [
        "print(input_embeddings.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}